# -*- coding: utf-8 -*-
"""ML_Crop_Recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10YkzG3DfUHmdT-vGyAm6LbNuytD4LB7f

# Importing the necessary libraries
"""

import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt

"""# Loading the dataset"""

data= pd.read_csv('/content/sample_data/Crop_recommendation.csv')

data.head()

"""# Data Exploration"""

data.describe

data.info

data.dtypes

data.isna().sum()

data.shape

data.apply(lambda x: len(x.unique()))

data['label'].value_counts()

"""# EDA:"""

f,ax=plt.subplots(figsize=(12,6))
corr=data.corr()
sns.heatmap(corr , ax=ax ,   cmap="rocket_r", annot=True)

data.columns

import warnings
warnings.filterwarnings("ignore")

f= plt.figure(figsize=(20,5))
ax=f.add_subplot(121)
sns.distplot(data['N'] , color ='green',ax=ax)

ax=f.add_subplot(122)
sns.distplot(data['P'] , color ='blue' , ax = ax)
plt.tight_layout()

f= plt.figure(figsize=(20,5))
ax=f.add_subplot(121)
sns.distplot(data['K'] , color ='green',ax=ax)

ax=f.add_subplot(122)
sns.distplot(data['temperature'] , color ='blue' , ax = ax)
plt.tight_layout()

f= plt.figure(figsize=(20,5))
ax=f.add_subplot(121)
sns.distplot(data['humidity'] , color ='green',ax=ax)

ax=f.add_subplot(122)
sns.distplot(data['ph'] , color ='blue' , ax = ax)
plt.tight_layout()

sns.distplot(data['rainfall'], color= 'blue')

sns.distplot(data['temperature'], color= 'black')

f= plt.figure(figsize=(15,5))
sns.countplot(data['label'] , palette = 'Spectral')
plt.xticks(rotation=90)
plt.show()

"""# Separating features and target into X and Y"""

features = data[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]
target = data['label']
labels = data['label']

acc = []
model = []

from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
DecisionTree = DecisionTreeClassifier(criterion="entropy",random_state=2,max_depth=5)

DecisionTree.fit(Xtrain,Ytrain)

predicted_values = DecisionTree.predict(Xtest)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Decision Tree')
print("DecisionTrees's Accuracy is: ", x*100)

print(classification_report(Ytest,predicted_values))

from sklearn.model_selection import cross_val_score

score = cross_val_score(DecisionTree, features, target,cv=5)

score

import pickle
DT_pkl_filename = 'DecisionTree.pkl'
DT_Model_pkl = open(DT_pkl_filename, 'wb')
pickle.dump(DecisionTree, DT_Model_pkl)

DT_Model_pkl.close()

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

LogReg = LogisticRegression(random_state=2)

LogReg.fit(Xtrain,Ytrain)

predicted_values = LogReg.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Logistic Regression')

print("Logistic Regression's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(LogReg,features,target,cv=5)

score

import pickle

LR_pkl_filename = 'LogisticRegression.pkl'

LR_Model_pkl = open(DT_pkl_filename, 'wb')
pickle.dump(LogReg, LR_Model_pkl)

LR_Model_pkl.close()

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier(n_estimators=20, random_state=0)
RF.fit(Xtrain,Ytrain)

predicted_values = RF.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('RF')

print("RF's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(RF,features,target,cv=5)

score

import pickle

RF_pkl_filename = 'RandomForest.pkl'

RF_Model_pkl = open(RF_pkl_filename, 'wb')
pickle.dump(RF, RF_Model_pkl)
RF_Model_pkl.close()

"""# XG Boost"""

import xgboost as xgb
XB = xgb.XGBClassifier()
XB.fit(Xtrain,Ytrain)

predicted_values = XB.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)

model.append('XGBoost')

print("XGBoost's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(XB,features,target,cv=5)
score

import pickle
XB_pkl_filename = 'XGBoost.pkl'

XB_Model_pkl = open(XB_pkl_filename, 'wb')
pickle.dump(XB, XB_Model_pkl)

XB_Model_pkl.close()

"""# SVM"""

from sklearn.svm import SVC
SVM = SVC(gamma='auto')

SVM.fit(Xtrain,Ytrain)

predicted_values = SVM.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)

model.append('SVM')
print("SVM's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(SVM,features,target,cv=5)
score

"""# Gaussian Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

NaiveBayes = GaussianNB()

NaiveBayes.fit(Xtrain,Ytrain)

predicted_values = NaiveBayes.predict(Xtest)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)

model.append('Naive Bayes')

print("Naive Bayes's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(NaiveBayes,features,target,cv=5)
score

import pickle
NB_pkl_filename = 'NBClassifier.pkl'
NB_Model_pkl = open(NB_pkl_filename, 'wb')
pickle.dump(NaiveBayes, NB_Model_pkl)

NB_Model_pkl.close()

plt.figure(figsize=[10,5],dpi = 100)

plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')

sns.barplot(x = acc,y = model)

accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, '-->', v)

"""# Making a prediction"""

data = np.array([[83, 45, 60, 28, 70.3, 7.0, 150.9]])
prediction = RF.predict(data)
print(prediction)